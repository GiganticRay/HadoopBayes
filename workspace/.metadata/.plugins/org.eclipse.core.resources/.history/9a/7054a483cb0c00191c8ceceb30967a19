import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.net.URI;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


public class BayesPredict
{
	// region define parameters
	public Configuration conf = new Configuration();	// 也可用 set 方法重新设置（会覆盖）: conf.set("fs.default.name", //"hdfs"//xxxx:9000) Configuration 类： 创建时会 | 读取 Hadoop 的配置文件，如 site-core.xml...;
	public double preciseCount = 0;						// 当前准确条数
	public double allCount = 0;							// 预测文件的总数
	public Map<String, Integer> featureFrequency = new HashMap<String, Integer>();
	// end region
	
	public void SetConfiguration(){
		
	}
	
    // 从HDFS中读取数据、并写入MAP
    public void readMapFromHdfs(String fileName){
        Path filePath=new Path(fileName);
        try {
            FileSystem fs=FileSystem.get(URI.create(fileName), conf);
            if(fs.exists(filePath)){
                String charset="UTF-8";             
                FSDataInputStream fsDataInputStream=fs.open(filePath);	//打开文件数据输入流            
                InputStreamReader inputStreamReader=new InputStreamReader(fsDataInputStream,charset);	//创建文件输入
                String line=null;         
                BufferedReader reader=null;								//把数据读入到缓冲区中
                reader=new BufferedReader(inputStreamReader);
                while((line=reader.readLine())!=null){					//从缓冲区中读取数据
                    String[] lineContent = line.toString().split("\t");
        			featureFrequency.put(lineContent[0], Integer.parseInt(lineContent[1]));	// 将其频数存放于全局变量
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
    
    
    // 从job2的输出文件统计准确率
    public void CountAccurate(String fileName){
    	Path filePath=new Path(fileName);
        try {
            FileSystem fs=FileSystem.get(URI.create(fileName),conf);
            if(fs.exists(filePath)){
                String charset="UTF-8";             
                FSDataInputStream fsDataInputStream=fs.open(filePath);	//打开文件数据输入流            
                InputStreamReader inputStreamReader=new InputStreamReader(fsDataInputStream,charset);	//创建文件输入
                String line=null;         
                BufferedReader reader=null;								//把数据读入到缓冲区中
                reader=new BufferedReader(inputStreamReader);
                while((line=reader.readLine())!=null){					//从缓冲区中读取数据
                	allCount ++;
                    String[] lineContent = line.toString().split("\t");
                    if(lineContent[1].equals(lineContent[2])){			// 说明预测准确
                    	preciseCount ++;
                    }
        			
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
	
	
	// 初始化全局参数
	public void InitGlobalPara(){
		preciseCount = 0;	// 归零
	}
	
	// 训练, 得到统计频数文件
	public void GetFrequencyFile(String[] otherArgs) throws Exception{
		Job trainJob = new Job(conf, "word count2");	// 新建一个 Job，传入配置信息	JobTrack 和 TaskTrack 都是 MRv1 用的东西了

		trainJob.setJarByClass(BayesClassifier.class);		// 设置主类
		trainJob.setMapperClass(BayesClassifier.TokenizerMapper.class);	// 设置 Mapper 类
		trainJob.setCombinerClass(BayesClassifier.IntSumReducer.class);	// 设置作业合成类	(就是在 map 之后、reduce 之前、要进行一次)
		trainJob.setReducerClass(BayesClassifier.IntSumReducer.class);	// 设置 Reducer 类
		trainJob.setOutputKeyClass(Text.class);			// 设置输出数据的关键类
		trainJob.setOutputValueClass(IntWritable.class);	// 设置输出值类
		
		// Train model, 就是统计每一个 特征属性 好评、差评 的频率
		FileInputFormat.addInputPath(trainJob, new Path(otherArgs[0]));	// 文件输入
		FileOutputFormat.setOutputPath(trainJob, new Path(otherArgs[1]));// 文件输出
		
		boolean flag = trainJob.waitForCompletion(true);
		System.out.print("SUCCEED!" + flag);
	}
	
	// 预测文件 JOB、输出预测的文件
	public void Predict() throws Exception{
		// countFrequencyJob: 判断输入文件中每一行的值
		Job predictJob = new Job(conf, "countFrequencyJob");
		predictJob.setJarByClass(BayesClassifier.class);		// 设置主类
		predictJob.setMapperClass(BayesClassifier.BayesPredictMapper.class);	// 设置 Mapper 类
		predictJob.setNumReduceTasks(0);   	 			//reduce的数量设为0
		
		FileInputFormat.addInputPath(predictJob, new Path("hdfs://192.168.10.100:9000/user/Hadoop/BayesData/test-1000.txt"));	// 文件输入
		FileOutputFormat.setOutputPath(predictJob, new Path("hdfs://192.168.10.100:9000/user/Hadoop/eclipseOutput4"));// 文件输出
		boolean flag = predictJob.waitForCompletion(true);
		System.out.println("GetFrequencyMap SUCCEED!" + flag);
		
		// 删除输出的文件夹、没有找到不产生输出文件的方法...
//				FileSystem fileSystem = FileSystem.get(new URI("hdfs://192.168.10.100:9000"), conf);
//				if (fileSystem.exists(new Path("/user/Hadoop/eclipseOutput4"))) {
//					fileSystem.delete(new Path("/user/Hadoop/eclipseOutput4"), true);
//					System.out.println("delete output file SUCCEED!");
//				}
	}
	
	public void GetAccurateFromPredictJob(){
		
	}
}
